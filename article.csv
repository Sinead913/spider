URL,webContent
https://en.wikipedia.org/wiki/High_availability,"
		From Wikipedia, the free encyclopedia
		
		
		
		
		
		Jump to navigation
		Jump to search
		""Always-on"" redirects here. For the software restriction, see Always-on DRM.


systems with high up-time, a.k.a. ""always on""

High availability (HA) is a characteristic of a system, which aims to ensure an agreed level of operational performance, usually uptime, for a higher than normal period.
Modernization has resulted in an increased reliance on these systems.  For example, hospitals and data centers require high availability of their systems to perform routine daily activities. Availability refers to the ability of the user community to obtain a service or good, access the system, whether to submit new work, update or alter existing work, or collect the results of previous work. If a user cannot access the system, it is – from the users point of view – unavailable.[1] Generally, the term downtime is used to refer to periods when a system is unavailable.

Contents

1 Principles
2 Scheduled and unscheduled downtime
3 Percentage calculation

3.1 ""Nines""


4 Measurement and interpretation
5 Closely related concepts
6 Military control systems
7 System design
8 Reasons for unavailability
9 Costs of unavailability
10 See also
11 Notes
12 References
13 External links



Principles[edit]
There are three principles of systems design in reliability engineering which can help achieve high availability.

Elimination of single points of failure. This means adding redundancy to the system so that failure of a component does not mean failure of the entire system.
Reliable crossover. In redundant systems, the crossover point itself tends to become a single point of failure.  Reliable systems must provide for reliable crossover.
Detection of failures as they occur. If the two principles above are observed, then a user may never see a failure – but the maintenance activity must.
Scheduled and unscheduled downtime[edit]
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: ""High availability"" – news · newspapers · books · scholar · JSTOR  (June 2008) (Learn how and when to remove this template message)
A distinction can be made between scheduled and unscheduled downtime. Typically, scheduled downtime is a result of maintenance that is disruptive to system operation and usually cannot be avoided with a currently installed system design. Scheduled downtime events might include patches to system software that require a reboot or system configuration changes that only take effect upon a reboot. In general, scheduled downtime is usually the result of some logical, management-initiated event. Unscheduled downtime events typically arise from some physical event, such as a hardware or software failure or environmental anomaly. Examples of unscheduled downtime events include power outages, failed CPU or RAM components (or possibly other failed hardware components), an over-temperature related shutdown, logically or physically severed network connections, security breaches, or various application, middleware, and operating system failures.
If users can be warned away from scheduled downtimes, then the distinction is useful.  But if the requirement is for true high availability, then downtime is downtime whether or not it is scheduled.
Many computing sites exclude scheduled downtime from availability calculations, assuming that it has little or no impact upon the computing user community. By doing this, they can claim to have phenomenally high availability, which might give the illusion of continuous availability. Systems that exhibit truly continuous availability are comparatively rare and higher priced, and most have carefully implemented specialty designs that eliminate any single point of failure and allow online hardware, network, operating system, middleware, and application upgrades, patches, and replacements. For certain systems, scheduled downtime does not matter, for example system downtime at an office building after everybody has gone home for the night.

Percentage calculation[edit]
Availability is usually expressed as a percentage of uptime in a given year. The following table shows the downtime that will be allowed for a particular percentage of availability, presuming that the system is required to operate continuously. Service level agreements often refer to monthly downtime or availability in order to calculate service credits to match monthly billing cycles. The following table shows the translation from a given availability percentage to the corresponding amount of time a system would be unavailable.




Availability %

Downtime per year[note 1]

Downtime per month

Downtime per week

Downtime per day


55.5555555% (""nine fives"")

162.33 days

13.53 days

74.92 hours

10.67 hours


90% (""one nine"")

36.53 days

73.05 hours

16.80 hours

2.40 hours


95% (""one and a half nines"")

18.26 days

36.53 hours

8.40 hours

1.20 hours


97%

10.96 days

21.92 hours

5.04 hours

43.20 minutes


98%

7.31 days

14.61 hours

3.36 hours

28.80 minutes


99% (""two nines"")

3.65 days

7.31 hours

1.68 hours

14.40 minutes


99.5% (""two and a half nines"")

1.83 days

3.65 hours

50.40 minutes

7.20 minutes


99.8%

17.53 hours

87.66 minutes

20.16 minutes

2.88 minutes


99.9% (""three nines"")

8.77 hours

43.83 minutes

10.08 minutes

1.44 minutes


99.95% (""three and a half nines"")

4.38 hours

21.92 minutes

5.04 minutes

43.20 seconds


99.99% (""four nines"")

52.60 minutes

4.38 minutes

1.01 minutes

8.64 seconds


99.995% (""four and a half nines"")

26.30 minutes

2.19 minutes

30.24 seconds

4.32 seconds


99.999% (""five nines"")

5.26 minutes

26.30 seconds

6.05 seconds

864.00 milliseconds


99.9999% (""six nines"")

31.56 seconds

2.63 seconds

604.80 milliseconds

86.40 milliseconds


99.99999% (""seven nines"")

3.16 seconds

262.98 milliseconds

60.48 milliseconds

8.64 milliseconds


99.999999% (""eight nines"")

315.58 milliseconds

26.30 milliseconds

6.05 milliseconds

864.00 microseconds


99.9999999% (""nine nines"")

31.56 milliseconds

2.63 milliseconds

604.80 microseconds

86.40 microseconds


Uptime and availability can be used synonymously as long as the items being discussed are kept consistent.  That is, a system can be up, but its services are not available, as in the case of a network outage.  This can also be viewed as a system that is available to be worked on, but its services are not up from a functional perspective (as opposed to software service/process perspective). The perspective is important here - whether the item being discussed is the server hardware, server OS, functional service, software service/process...etc.  Keep the perspective consistent throughout a discussion, then uptime and availability can be used synonymously.

""Nines""[edit]
Main article: Nine (purity)
Percentages of a particular order of magnitude are sometimes referred to by the number of nines or ""class of nines"" in the digits.  For example, electricity that is delivered without interruptions (blackouts, brownouts or surges) 99.999% of the time would have 5 nines reliability, or class five.[2] In particular, the term is used in connection with mainframes[3][4] or enterprise computing, often as part of a service-level agreement.
Similarly, percentages ending in a 5 have conventional names, traditionally the number of nines, then ""five"", so 99.95% is ""three nines five"", abbreviated 3N5.[5][6] This is casually referred to as ""three and a half nines"",[7] but this is incorrect: a 5 is only a factor of 2, while a 9 is a factor of 10, so a 5 is 0.3 nines (per below formula: 
  
    
      
        
          log
          
            10
          
        
        ⁡
        2
        ≈
        0.3
      
    
    {\displaystyle \log _{10}2\approx 0.3}
  
):[note 2] 99.95% availability is 3.3 nines, not 3.5 nines.[8] More simply, going from 99.9% availability to 99.95% availability is a factor of 2 (0.1% to 0.05% unavailability), but going from 99.95% to 99.99% availability is a factor of 5 (0.05% to 0.01% unavailability), over twice as much.[note 3]
A formulation of the class of 9s  
  
    
      
        c
      
    
    {\displaystyle c}
  
  based on a system's unavailability 
  
    
      
        x
      
    
    {\displaystyle x}
  
 would be


  
    
      
        c
        :=
        ⌊
        −
        
          log
          
            10
          
        
        ⁡
        x
        ⌋
      
    
    {\displaystyle c:=\lfloor -\log _{10}x\rfloor }
  

(cf. Floor and ceiling functions).
A similar measurement is sometimes used to describe the purity of substances.
In general, the number of nines is not often used by a network engineer when modeling and measuring availability because it is hard to apply in formula. More often, the unavailability expressed as a probability (like 0.00001), or a downtime per year is quoted. Availability specified as a number of nines is often seen in marketing documents.[citation needed] The use of the ""nines"" has been called into question, since it does not appropriately reflect that the impact of unavailability varies with its time of occurrence.[9] For large amounts of 9s, the ""unavailability"" index (measure of downtime rather than uptime) is easier to handle. For example, this is why an ""unavailability"" rather than availability metric is used in hard disk or data link bit error rates.

Measurement and interpretation[edit]
Availability measurement is subject to some degree of interpretation. A system that has been up for 365 days in a non-leap year might have been eclipsed by a network failure that lasted for 9 hours during a peak usage period; the user community will see the system as unavailable, whereas the system administrator will claim 100% uptime. However, given the true definition of availability, the system will be approximately 99.9% available, or three nines (8751 hours of available time out of 8760 hours per non-leap year). Also, systems experiencing performance problems are often deemed partially or entirely unavailable by users, even when the systems are continuing to function. Similarly, unavailability of select application functions might go unnoticed by administrators yet be devastating to users — a true availability measure is holistic.
Availability must be measured to be determined, ideally with comprehensive monitoring tools (""instrumentation"") that are themselves highly available. If there is a lack of instrumentation, systems supporting high volume transaction processing throughout the day and night, such as credit card processing systems or telephone switches, are often inherently better monitored, at least by the users themselves, than systems which experience periodic lulls in demand.
An alternative metric is mean time between failures (MTBF).

Closely related concepts[edit]
Recovery time (or estimated time of repair (ETR), also known as recovery time objective (RTO) is closely related to availability, that is the total time required for a planned outage or the time required to fully recover from an unplanned outage. Another metric is mean time to recovery (MTTR).  Recovery time could be infinite with certain system designs and failures, i.e. full recovery is impossible. One such example is a fire or flood that destroys a data center and its systems when there is no secondary disaster recovery data center.
Another related concept is data availability, that is the degree to which databases and other information storage systems faithfully record and report system transactions. Information management often focuses separately on data availability, or Recovery Point Objective, in order to determine acceptable (or actual) data loss with various failure events. Some users can tolerate application service interruptions but cannot tolerate data loss.
A service level agreement (""SLA"") formalizes an organization's availability objectives and requirements.

Military control systems[edit]
High availability is one of the primary requirements of the control systems in unmanned vehicles and autonomous maritime vessels. If the controlling system becomes unavailable, the Ground Combat Vehicle (GCV) or ASW Continuous Trail Unmanned Vessel (ACTUV) would be lost.

System design[edit]
Adding more components to an overall system design can undermine efforts to achieve high availability because complex systems inherently have more potential failure points and are more difficult to implement correctly. While some analysts would put forth the theory that the most highly available systems adhere to a simple architecture (a single, high quality, multi-purpose physical system with comprehensive internal hardware redundancy), this architecture suffers from the requirement that the entire system must be brought down for patching and operating system upgrades. More advanced system designs allow for systems to be patched and upgraded without compromising service availability (see load balancing and failover).
High availability requires less human intervention to restore operation in complex systems; the reason for this being that the most common cause for outages is human error.[10]
Redundancy is used to create systems with high levels of availability (e.g. aircraft flight computers). In this case it is required to have high levels of failure detectability and avoidance of common cause failures. Two kinds of redundancy are passive redundancy and active redundancy.
Passive redundancy is used to achieve high availability by including enough excess capacity in the design to accommodate a performance decline. The simplest example is a boat with two separate engines driving two separate propellers. The boat continues toward its destination despite failure of a single engine or propeller. A more complex example is multiple redundant power generation facilities within a large system involving electric power transmission. Malfunction of single components is not considered to be a failure unless the resulting performance decline exceeds the specification limits for the entire system.
Active redundancy is used in complex systems to achieve high availability with no performance decline. Multiple items of the same kind are incorporated into a design that includes a method to detect failure and automatically reconfigure the system to bypass failed items using a voting scheme. This is used with complex computing systems that are linked. Internet routing is derived from early work by Birman and Joseph in this area.[11] Active redundancy may introduce more complex failure modes into a system, such as continuous system reconfiguration due to faulty voting logic.
Zero downtime system design means that modeling and simulation indicates mean time between failures significantly exceeds the period of time between planned maintenance, upgrade events, or system lifetime. Zero downtime involves massive redundancy, which is needed for some types of aircraft and for most kinds of communications satellites. Global Positioning System is an example of a zero downtime system.
Fault instrumentation can be used in systems with limited redundancy to achieve high availability. Maintenance actions occur during brief periods of down-time only after a fault indicator activates. Failure is only significant if this occurs during a mission critical period.
Modeling and simulation is used to evaluate the theoretical reliability for large systems. The outcome of this kind of model is used to evaluate different design options. A model of the entire system is created, and the model is stressed by removing components. Redundancy simulation involves the N-x criteria. N represents the total number of components in the system. x is the number of components used to stress the system. N-1 means the model is stressed by evaluating performance with all possible combinations where one component is faulted. N-2 means the model is stressed by evaluating performance with all possible combinations where two component are faulted simultaneously.

Reasons for unavailability[edit]
A survey among academic availability experts in 2010 ranked reasons for unavailability of enterprise IT systems. All reasons refer to not following best practice in each of the following areas (in order of importance):[12]

Monitoring of the relevant components
Requirements and procurement
Operations
Avoidance of network failures
Avoidance of internal application failures
Avoidance of external services that fail
Physical environment
Network redundancy
Technical solution of backup
Process solution of backup
Physical location
Infrastructure redundancy
Storage architecture redundancy
A book on the factors themselves was published in 2003.[13]

Costs of unavailability[edit]
In a 1998 report from IBM Global Services, unavailable systems were estimated to have cost American businesses $4.54 billion in 1996, due to lost productivity and revenues.[14]

See also[edit]
Disaster recovery
Fault-tolerance
Reliability, availability and serviceability (computing)
Reliability engineering
Resilience (network)
Ubiquitous computing
Notes[edit]


^ Using 365.25 days per year. For consistency, all times are rounded to two decimal digits.

^ See mathematical coincidences concerning base 2 for details on this approximation.

^ ""Twice as much"" on a logarithmic scale, meaning two factors of 2: 
  
    
      
        ×
        2
        ×
        2
        <
        ×
        5
      
    
    {\displaystyle \times 2\times 2<\times 5}
  



References[edit]


^ Floyd Piedad, Michael Hawkins (2001). High Availability: Design, Techniques, and Processes. Prentice Hall. ISBN 9780130962881..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:""\""""""\""""""'""""'""}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ Lecture Notes M. Nesterenko, Kent State University

^ Introduction to the new mainframe: Large scale commercial computing Chapter 5 Availability IBM (2006)

^ IBM zEnterprise EC12 Business Value Video at youtube.com

^ Precious metals, Volume 4. Pergamon Press. 1981. p. page 262. ISBN 9780080253695.

^ PVD for Microelectronics: Sputter Desposition to Semiconductor Manufacturing. 1998. p. 387.

^ Murphy, Niall Richard; Beyer, Betsy; Petoff, Jennifer; Jones, Chris (2016). Site Reliability Engineering: How Google Runs Production Systems. p. 38.

^ Josh Deprez (April 23, 2016). ""Nines of Nines"".

^ Evan L. Marcus, The myth of the nines

^ ""Top Seven Considerations for Configuration Management for Virtual and Cloud Infrastructures"". Gartner. October 27, 2010. Retrieved October 13, 2013.

^ RFC 992

^ Ulrik Franke, Pontus Johnson, Johan König, Liv Marcks von Würtemberg: Availability of enterprise IT systems – an expert-based Bayesian model, Proc. Fourth International Workshop on Software Quality and Maintainability (WSQM 2010), Madrid, [1]

^ Marcus, Evan; Stern, Hal (2003). Blueprints for high availability (Second ed.). Indianapolis, IN: John Wiley & Sons. ISBN 0-471-43026-9.

^ IBM Global Services, Improving systems availability, IBM Global Services, 1998, [2]


External links[edit]
Lecture Notes on Enterprise Computing University of Tübingen
Uptime Calculator (SLA)





		
		Retrieved from ""https://en.wikipedia.org/w/index.php?title=High_availability&oldid=933004783""
		
		Categories: System administrationQuality controlApplied probabilityReliability engineeringMeasurementHidden categories: Use American English from March 2019All Wikipedia articles written in American EnglishArticles with short descriptionUse mdy dates from March 2019Articles needing additional references from June 2008All articles needing additional referencesAll articles with unsourced statementsArticles with unsourced statements from August 2008
		
		
	"
https://en.wikipedia.org/wiki/FreeNATS,"
		From Wikipedia, the free encyclopedia
		
		
		
		
		
		Jump to navigation
		Jump to search
		FreeNATSOriginal author(s)David CuttingDeveloper(s)PurplePixie SystemsInitial releaseMarch 2, 2008 (2008-03-02)[1]Stable release1.20.1b
   / November 14, 2018; 13 months ago (2018-11-14)[2]
Operating systemUnix-likePlatformPHP / MySQLAvailable inEnglishTypeNetwork monitoringLicenseGNU General Public LicenseWebsitewww.purplepixie.org/freenats/
FreeNATS (the Free Network Automatic Testing System) is an open-source network monitoring software application[3] developed by David Cutting under the banner of PurplePixie Systems.
FreeNATS is free software licensed under the terms of the GNU General Public License version 3 as published by the Free Software Foundation.

Contents

1 Overview
2 See also
3 References
4 External links



Overview[edit]
Monitoring of network services (SMTP, POP3, HTTP, ICMP (ping)[4]
Limited monitoring of host resources (processor load, disk usage) on a majority of network operating systems, including Microsoft Windows and Linux through agent-based testing
Plugin design that allows users to easily develop their own service checks depending on needs, by using PHP (or other languages or scripts wrapped in PHP)
Some ability to define network host hierarchy using ""master"" nodes allowing link failures to suspend monitoring[5]
Event-based system allowing failure notifications to be sent in customised email (suitable for email-to-SMS) or to utilise third-party notification scripts via a plug-in
Ability to define event handlers to be run during service or host events for proactive problem resolution
Automatic data retention cleanups
Full web-interface for management and monitoring
Ability to ""publish"" views and graphs within third-party web pages
See also[edit]
Comparison of network monitoring systems
References[edit]


^ first release referenced in http://www.purplepixie.org/freenats/news.php?id=91

^ FreeNATS Release Download

^ NetworkWorld.com Article on FreeNATS

^ PC Quest Article Archived 2009-02-10 at the Wayback Machine on FreeNATS

^ Master Node Documentation on FreeNATS Wiki


External links[edit]
purplepixie.org/freenats, official website
FreeNATS Wiki
Support Forum for FreeNATS
NetworkWorld.com Article on FreeNATS
PC Quest Article on FreeNATS





		
		Retrieved from ""https://en.wikipedia.org/w/index.php?title=FreeNATS&oldid=931812881""
		
		Categories: Network managementInternet Protocol based network softwareFree network management softwareHidden categories: Webarchive template wayback links
		
		
	"
https://en.wikipedia.org/wiki/Network_traffic_measurement,"
		From Wikipedia, the free encyclopedia
		
		
		
		
		
		Jump to navigation
		Jump to search
		In computer networks, network traffic measurement is the process of measuring the amount and type of traffic on a particular network. This is especially important with regard to effective bandwidth management.

Contents

1 Techniques
2 Measurement studies
3 Tools

3.1 Functions and features


4 See also
5 References
6 External links



Techniques[edit]
Network performance could be measured using either active or passive techniques. Active techniques (e.g. Iperf) are more intrusive but are arguably more accurate. Passive techniques have less network overhead and hence can run in the background to be used to trigger network management actions.

Measurement studies[edit]
A range of studies have been performed from various points on the Internet. The AMS-IX (Amsterdam Internet Exchange) is one of the world's largest Internet exchanges. It produces a constant supply of simple Internet statistics. There are also numerous academic studies that have produced a range of measurement studies[1][2][3] on frame size distributions, TCP/UDP ratios and TCP/IP options.

Tools[edit]
Various software tools are available to measure network traffic. Some tools measure traffic by sniffing and others use SNMP, WMI or other local agents to measure bandwidth use on individual machines and routers. However, the latter generally do not detect the type of traffic, nor do they work for machines which are not running the necessary agent software, such as rogue machines on the network, or machines for which no compatible agent is available. In the latter case, inline appliances are preferred. These would generally 'sit' between the LAN and the LAN's exit point, generally the WAN or Internet router, and all packets leaving and entering the network would go through them. In most cases the appliance would operate as a bridge on the network so that it is undetectable by users.
Some tools used for SNMP monitoring are InfoVista[4], Tivoli Netcool/Proviso [5] by IBM, CA Performance Management by CA Technologies[6], TotalView [7] by PathSolutions, and SolarWinds[8].

Functions and features[edit]
Measurement tools generally have these functions and features:

User interface (web, graphical, console)
Real-time traffic graphs
Network activity is often reported against pre-configured traffic matching rules to show:
Local IP address
Remote IP address
Port number or protocol
Logged in user name
Bandwidth quotas
Support for traffic shaping or rate limiting (overlapping with the network traffic control page)
Support website blocking and content filtering
Alarms to notify the administrator of excessive usage (by IP address or in total)
See also[edit]
IP Flow Information Export and NetFlow
Measuring network throughput
Network management
Network monitoring
Network scheduler
Network simulation
Packet sniffer
Performance management
References[edit]


^ Murray, David; Terry Koziniec (2012). ""The State of Enterprise Network Traffic in 2012"". 18th Asia-Pacific Conference on Communications (APCC 2012)..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:""\""""""\""""""'""""'""}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ Zhang, Min; Maurizio Dusi; Wolfgang John; Changjia Chen (2009). ""Analysis of udp traffic usage on internet backbone links"". In Proceedings of the 2009 Ninth Annual International Symposium on Applications and the Internet.

^ Wolfgang, John; Sven Tafvelin (2007). ""Analysis of internet backbone traffic and header anomalies observed"". ACM Wireless Networks. Proceedings of the 7th ACM SIGCOMM conference on Internet measurement.

^ ""Product Overview"". InfoVista.com. Retrieved 27 September 2018.

^ ""Configuring IBM Tivoli Storage Manager SNMP"". ibm.com. Retrieved 27 September 2018.

^ ""CA Performance Management - 2.8"". docops.ca.com. Retrieved 27 September 2018.

^ ""Selecting The Right Network Troubleshooting Tool Part Three: SNMP"". PathSolutions.com. Retrieved 27 September 2018.

^ ""SNMP Monitoring"". SolarWinds.com. Retrieved 27 September 2018.


External links[edit]
AMS-IX Internet Statistics
Network Event Detection With Entropy Measures, Dr. Raimund Eimann, University of Auckland, PDF; 5993 kB





		
		Retrieved from ""https://en.wikipedia.org/w/index.php?title=Network_traffic_measurement&oldid=933786062""
		
		Categories: Network managementInternet Protocol based network software
		
		
	"
https://en.wikipedia.org/wiki/Service-level_agreement,"
		From Wikipedia, the free encyclopedia
		
		
		
		
		
		Jump to navigation
		Jump to search
		For other uses, see SLA (disambiguation).
A service-level agreement (SLA) is a commitment between a service provider and a client. Particular aspects of the service – quality, availability, responsibilities – are agreed between the service provider and the service user.[1] The most common component of an SLA is that the services should be provided to the customer as agreed upon in the contract. As an example, Internet service providers and telcos will commonly include service level agreements within the terms of their contracts with customers to define the level(s) of service being sold in plain language terms. In this case the SLA will typically have a technical definition in  mean time between failures (MTBF), mean time to repair or mean time to recovery (MTTR); identifying which party is responsible for reporting faults or paying fees; responsibility for various data rates; throughput; jitter; or similar measurable details.

Contents

1 Overview
2 Components
3 Common metrics
4 Specific examples

4.1 Backbone Internet providers
4.2 WSLA
4.3 Cloud computing
4.4 Outsourcing


5 See also
6 References
7 External links



Overview[edit]
A service-level agreement is an agreement between two or more parties, where one is the customer and the others are service providers. This can be a legally binding formal or an informal ""contract"" (for example, internal department relationships). The agreement may involve separate organizations, or different teams within one organization. Contracts between the service provider and other third parties are often (incorrectly) called SLAs – because the level of service has been set by the (principal) customer, there can be no ""agreement"" between third parties; these agreements are simply ""contracts."" Operational-level agreements or OLAs, however, may be used by internal groups to support SLAs. If some aspect of a service has not been agreed with the customer, it is not an ""SLA"".
SLAs commonly include many components, from a definition of services to the termination of agreement.[2] To ensure that SLAs are consistently met, these agreements are often designed with specific lines of demarcation and the parties involved are required to meet regularly to create an open forum for communication. Rewards and penalties applying to the provider are often specified.  Most SLAs also leave room for periodic (annual) revisitation to make changes.[3]
Since late 1980s SLAs have been used by fixed line telecom operators. SLAs are so widely used these days that larger organizations have many different SLAs existing within the company itself. Two different units in an organization script a SLA with one unit being the customer and another being the service provider. This practice helps to maintain the same quality of service amongst different units in the organization and also across multiple locations of the organization. This internal scripting of SLA also helps to compare the quality of service between an in-house department and an external service provider.[4]
The output received by the customer as a result of the service provided is the main focus of the service level agreement.
Service level agreements are also defined at different levels:

Customer-based SLA: An agreement with an individual customer group, covering all the services they use. For example, an SLA between a supplier (IT service provider) and the finance department of a large organization for the services such as finance system, payroll system, billing system, procurement/purchase system, etc.
Service-based SLA: An agreement for all customers using the services being delivered by the service provider. For example:
A mobile service provider offers a routine service to all the customers and offers certain maintenance as a part of an offer with the universal charging.
An email system for the entire organization. There are chances of difficulties arising in this type of SLA as level of the services being offered may vary for different customers (for example, head office staff may use high-speed LAN connections while local offices may have to use a lower speed leased line).
Multilevel SLA: The SLA is split into the different levels, each addressing different set of customers for the same services, in the same SLA.
Corporate-level SLA: Covering all the generic service level management (often abbreviated as SLM) issues appropriate to every customer throughout the organization. These issues are likely to be less volatile and so updates (SLA reviews) are less frequently required.
Customer-level SLA: covering all SLM issues relevant to the particular customer group, regardless of the services being used.
Service-level SLA: covering all SLM issue relevant to the specific services, in relation to this specific customer group.
Components[edit]
A well defined and typical SLA will contain the following components:[5]

Type of service to be provided: It specifies the type of service and any additional details of type of service to be provided. In case of an IP network connectivity, type of service will describe functions such as operation and maintenance of networking equipment, connection bandwidth to be provided, etc.
The service's desired performance level, especially its reliability and responsiveness: A reliable service will be the one which suffers minimum disruptions in a specific amount of time and is available at almost all times. A service with good responsiveness will perform the desired action promptly after the customer requests for it.
Monitoring process and service level reporting: This component describes how the performance levels are supervised and monitored. This process involves gathering of different type of statistics, how frequently this statistics will be collected and how this statistics will be accessed by the customers.
The steps for reporting issues with the service: This component will specify the contact details to report the problem to and the order in which details about the issue have to be reported. The contract will also include a time range in which the problem will be looked upon and also till when the issue will be resolved.
Response and issue resolution time-frame: Response time-frame is the time period by which the service provider will start the investigation of the issue. Issue resolution time-frame is the time period by which the current service issue will be resolved and fixed.
Repercussions for service provider not meeting its commitment: If the provider is not able to meet the requirements as stated in SLA then service provider will have to face consequences for the same. These consequences may include customer's right to terminate the contract or ask for a refund for losses incurred by the customer due to failure of service.
Common metrics[edit]
Service-level agreements can contain numerous service-performance metrics with corresponding service-level objectives. A common case in IT-service management is a call center or service desk. Metrics commonly agreed to in these cases include:

Abandonment Rate: Percentage of calls abandoned while waiting to be answered.
ASA (Average Speed to Answer): Average time (usually in seconds) it takes for a call to be answered by the service desk.
TSF (Time Service Factor): Percentage of calls answered within a definite timeframe, e.g., 80% in 20 seconds.
FCR (First-Call Resolution): Percentage of incoming calls that can be resolved without the use of a callback or without having the caller call back the helpdesk to finish resolving the case.[6]
TAT (Turn-Around Time): Time taken to complete a certain task.
TRT (total resolution time): Total time taken to complete a certain task.
MTTR (Mean Time To Recover): Time taken to recover after an outage of service.
Uptime is also a common metric, often used for data services such as shared hosting, virtual private servers and dedicated servers. Common agreements include percentage of network uptime, power uptime, number of scheduled maintenance windows, etc.
Many SLAs track to the Information Technology Infrastructure Library specifications when applied to IT services.

Specific examples[edit]
Backbone Internet providers[edit]
It is not uncommon for an internet backbone service provider (or network service provider) to explicitly state its own SLA on its website.[7][8][9] The U.S. Telecommunications Act of 1996 does not expressly mandate that companies have SLAs, but it does provide a framework for firms to do so in Sections 251 and 252.[10] Section 252(c)(1) for example (""Duty to Negotiate"") requires Incumbent local exchange carriers (ILECs) to negotiate in good faith about matters such as resale and access to rights of way.

WSLA[edit]
A web service level agreement (WSLA) is a standard for service level agreement compliance monitoring of web services. It allows authors to specify the performance metrics associated with a web service application, desired performance targets, and actions that should be performed when performance is not met.
WSLA Language Specification, version 1.0 was published by IBM on January 28, 2001.

Cloud computing[edit]
The underlying benefit of cloud computing is shared resources, which is supported by the underlying nature of a shared infrastructure environment. Thus, SLAs span across the cloud and are offered by service providers as a service-based agreement rather than a customer-based agreement. Measuring, monitoring and reporting on cloud performance is based on the end UX or their ability to consume resources. The downside of cloud computing relative to SLAs is the difficulty in determining the root cause of service interruptions due to the complex nature of the environment.
As applications are moved from dedicated hardware into the cloud, they need to achieve the same or even more demanding levels of service than classical installations. SLAs for cloud services focus on characteristics of the data center and more recently include characteristics of the network (see carrier cloud) to support end-to-end SLAs.[11]
Any SLA management strategy considers two well-differentiated phases: negotiating the contract and monitoring its fulfilment in real time. Thus, SLA management encompasses the SLA contract definition: the basic schema with the QoS parameters; SLA negotiation; SLA monitoring; SLA violation detection; and SLA enforcement—according to defined policies.
The main point is to build a new layer upon the grid, cloud, or SOA middleware able to create a negotiation mechanism between the providers and consumers of services. An example is the EU–funded Framework 7 research project, SLA@SOI,[12] which is researching aspects of multi-level, multi-provider SLAs within service-oriented infrastructure and cloud computing, while another EU-funded project, VISION Cloud,[13] has provided results with respect to content-oriented SLAs.
FP7 IRMOS also investigated aspects of translating application-level SLA terms to resource-based attributes in an effort to bridge the gap between client-side expectations and cloud-provider resource-management mechanisms.[14][15] A summary of the results of various research projects in the area of SLAs (ranging from specifications to monitoring, management and enforcement) has been provided by the European Commission.[16]

Outsourcing[edit]
Outsourcing involves the transfer of responsibility from an organization to a supplier. This new arrangement is managed through a contract that may include one or more SLAs. The contract may involve financial penalties and the right to terminate if any of the SLAs metrics are consistently missed. Setting, tracking and managing SLAs is an important part of the outsourcing relationship management (ORM) discipline. Specific SLAs are typically negotiated up front as part of the outsourcing contract and used as one of the primary tools of outsourcing governance.
In software development, specific SLAs can apply to application outsourcing contracts in line with standards in software quality, as well as recommendations provided by neutral organizations like CISQ, which has published numerous papers on the topic (such as Using Software Measurement in SLAs[17]) that are available to the public.

See also[edit]
Best-effort delivery
IT cost transparency
Network monitoring
Operational-level agreement (OLA)
Service-oriented architecture (SOA)
Service level
Service level objective
Service level requirement
References[edit]


^ Kearney, K.T.; Torelli, F. (2011). ""The SLA Model"".  In Wieder, P.; Butler, J.M.; Theilmann, W.; Yahyapour, R. (eds.). Service Level Agreements for Cloud Computing. Springer Science+Business Media, LLC. pp. 43–68. ISBN 9781461416142..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:""\""""""\""""""'""""'""}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(""//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png"")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}

^ ""The Service Level Agreement Zone"". SLA Information Zone. Service Level Agreement Zone. 2015. Retrieved 22 June 2016.

^ Shacklett, M.E. (12 January 2011). ""Five Key Points for Every SLA"". Dell. Archived from the original on 22 December 2012. Retrieved 22 June 2016.

^ Ding, Jianguo (2010). Advances in Network Management. Auerbach Publications. ISBN 978-1-4200-6455-1.

^ Verma, Dinesh (September 2004). ""Service level agreements on IP networks"" (PDF). 92 (9). Cite journal requires |journal= (help)

^ Desmarais, Mike (2012). ""First Call Resolution"" (PDF).

^ ""Global IP Network SLA"". NTT Communications. Retrieved 22 June 2016.

^ ""Global Latency and Packet Delivery SLA"". Verizon. Retrieved 22 June 2016.

^ ""Business Edition - AT&T U-verse Voice and TV - Terms of Service (TOS) and AT&T Broadband - Service Level Agreement (SLA)"". AT&T. Retrieved 22 June 2016.

^ Wikisource:Telecommunications Act of 1996#SEC. 101. ESTABLISHMENT OF PART II OF TITLE II.

^ Rueda, J.L.; Gómez, S.G.; Chimento, A.E. (2011). ""The Service Aggregator Use Case Scenario"".  In Wieder, P.; Butler, J.M.; Theilmann, W.; Yahyapour, R. (eds.). Service Level Agreements for Cloud Computing. Springer Science+Business Media, LLC. pp. 329–342. ISBN 9781461416142.

^ Butler, J.M.; Yahyapour, R.; Theilmann, W. (2011). ""Motivation and Overview"".  In Wieder, P.; Butler, J.M.; Theilmann, W.; Yahyapour, R. (eds.). Service Level Agreements for Cloud Computing. Springer Science+Business Media, LLC. pp. 3–12. ISBN 9781461416142.

^ Villari, M.; Tusa, F.; Celesti, A.; Puliafito, A. (2012). ""How to Federate VISION Cloud through SAML/Shibboleth Authentication"".  In De Paoli, F.; Pimentel, E.; Zavattaro, G. (eds.). Service-Oriented and Cloud Computing. Springer-Verlag Berlin Heidelberg. pp. 259–274. ISBN 9783642334276.

^ Boniface, M.; Nasser, B.; Papay, J.;  et al. (2010). ""Platform-as-a-Service Architecture for Real-Time Quality of Service Management in Clouds"" (PDF). ICIW '10: Proceedings of the 2010 Fifth International Conference on Internet and Web Applications and Services: 155–160. doi:10.1109/ICIW.2010.91. ISBN 978-1-4244-6728-0.

^ Cuomo, A.; Di Modica, G.; Distefano, S.;  et al. (2013). ""An SLA-based Broker for Cloud Infrastructures"". Journal of Grid Computing. 11 (March 2013): 1–25. doi:10.1007/s10723-012-9241-4.

^ Kyriazis, D., ed. (June 2013). ""Cloud Computing Service Level Agreements - Exploitation of Research Results"". European Commission. p. 51. Retrieved 22 June 2016.

^ Curtis, B.; Herron, D.; Subramanyam, J. (July 2015). ""Using Software Measurement in SLAs: Integrating CISQ Size and Structural Quality Measures into Contractual Relationships"" (PDF). CISQ. Retrieved 22 June 2016.


External links[edit]
Service Level Agreement (SLA) S-Cube Knowledge Model





		
		Retrieved from ""https://en.wikipedia.org/w/index.php?title=Service-level_agreement&oldid=928173132""
		
		Categories: Contract lawOutsourcingIT service managementTerms of serviceServices marketingHidden categories: CS1 errors: missing periodical
		
		
	"
https://en.wikipedia.org/wiki/Network_monitoring,"
		From Wikipedia, the free encyclopedia
		
		
		
		
		
		Jump to navigation
		Jump to search
		This article is about monitoring for technical failures. For monitoring in the sense of surveillance, but relying mostly on the same technology, see Computer and network surveillance.
Network monitoring is the use of a system that constantly monitors a computer network for slow or failing components and that notifies the network administrator (via email, SMS or other alarms) in case of outages or other trouble. Network monitoring is part of network management.

Contents

1 Details
2 Network tomography
3 Route analytics
4 Various types of protocols
5 Internet server monitoring

5.1 Servers around the globe
5.2 Web server monitoring process
5.3 Notification


6 See also
7 Notes and references
8 External links



Details[edit]
While an intrusion detection system monitors a network threats from the outside, a network monitoring system monitors the network for problems caused by overloaded or crashed servers, network connections or other devices.
For example, to determine the status of a web server, monitoring software may periodically send an HTTP request to fetch a page. For email servers, a test message might be sent through SMTP and retrieved by IMAP or POP3.
Commonly measured metrics are response time, availability and uptime, although both consistency and reliability metrics are starting to gain popularity. The widespread addition of WAN optimization devices is having an adverse effect on most network monitoring tools, especially when it comes to measuring accurate end-to-end delay because they limit round-trip delay time visibility.[1]
Status request failures, such as when a connection cannot be established, it times-out, or the document or message cannot be retrieved, usually produce an action from the monitoring system. These actions vary; An alarm may be sent (via SMS, email, etc.) to the resident sysadmin, automatic failover systems may be activated to remove the troubled server from duty until it can be repaired, etc.
Monitoring the performance of a network uplink is also known as network traffic measurement.

Network tomography[edit]
Network tomography is an important area of network measurement, which deals with monitoring the health of various links in a network using end-to-end probes sent by agents located at vantage points in the network/Internet.

Route analytics[edit]
Route analytics is another important area of network measurement. It includes
the methods, systems, algorithms and tools to monitor the routing posture of networks. Incorrect routing or routing issues cause undesirable performance degradation or downtime.

Various types of protocols[edit]
Site monitoring services can check HTTP pages, HTTPS, SNMP, FTP, SMTP, POP3, IMAP, DNS, SSH, TELNET, SSL, TCP, ICMP, SIP, UDP, Media Streaming and a range of other ports with a variety of check intervals ranging from every four hours to every one minute. Typically, most network monitoring services test your server anywhere between once-per-hour to once-per-minute.

Internet server monitoring[edit]
See also: Website monitoring
Monitoring an internet server means that the server owner always knows if one or all of his services go down. Server monitoring may be internal, i.e. web server software checks its status and notifies the owner if some services go down, and external, i.e. some web server monitoring companies check the services status with a certain frequency. Server monitoring can encompass a check of system metrics, such as CPU usage, memory usage, network performance and disk space. It can also include application monitoring, such as checking the processes of programs such as Apache, MySQL, Nginx, Postgres and others.
External monitoring is more reliable, as it keeps on working when the server completely goes down. Good server monitoring tools also have performance benchmarking, alerting capabilities and the ability to link certain thresholds with automated server jobs, such as provisioning more memory or performing a backup.

Servers around the globe[edit]
Network monitoring services usually have a number of servers around the globe - for example in America, Europe, Asia, Australia and other locations. By having multiple servers in different geographic locations, a monitoring service can determine if a Web server is available across different networks worldwide. The more the locations used, the more complete is the picture on network availability.

Web server monitoring process[edit]
When monitoring a web server for potential problems, an external web monitoring service checks a number of parameters. First of all, it monitors for a proper HTTP return code. By HTTP specifications RFC 2616, any web server returns several HTTP codes. Analysis of the HTTP codes is the fastest way to determine the current status of the monitored web server. Third-party application performance monitoring tools  provide additional web server monitoring, alerting and reporting capabilities.

Notification[edit]
As the information brought by web server monitoring services is in most cases urgent and may be of crucial importance, various notification methods may be used: e-mail, land-line and cell phones, messengers, SMS, fax, pagers, etc.

See also[edit]
Business service management
Comparison of network monitoring systems
High availability
Network Monitoring Interface Card
Network traffic measurement
Network tap
System monitor
Service-level agreement
Notes and references[edit]


^ The impact of WAN Optimization on NetFlow/IPFIX measurements


External links[edit]



Wikiversity has learning resources about  Network monitoring

Network Management at Curlie
List of Network Monitoring and Management Tools at Stanford University 





		
		Retrieved from ""https://en.wikipedia.org/w/index.php?title=Network_monitoring&oldid=929348670""
		
		Categories: Network managementHidden categories: Articles with Curlie linksPages using RFC magic links
		
		
	"
